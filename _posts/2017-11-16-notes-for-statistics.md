Statistics, David Freedman
### 1. Controlled Experiments
method of comparison  
double-blind both subjects don't know whether they are in treatment or in  
control; neither do those who evaluate the responses.  

### 2. Observational Studies
hidden confounders are a major problem in observational studies.  
confounding means a difference between the treatment and control groups-
other than the treatment-which affects the responses being studied.  

### 3. The Histogram

### 4. The Average and the Standard Deviation

### 5. The Normal Approximation for Data

### 6. Measurement Error

### 7. Plotting Points and Lines

### 8. Correlation
correlation coefficient a measure of linear association, or clustering
around a line.  
standard units f(average, standard deviation)  
SD line goes through the point of averages, and climbs at the rate of one
vertical SD for each horizontal SD. The slope is the ratio (SD of y)/(SD
of x).  
correlation coefficient r = average of (x in standard units) * (y in
standard units).  

### 9. More about Correlation
The correlation coefficient is a pure number, without units.  
The appearance of a scatter diagram depends on the SDs.  
The correlation coefficient is useful for football-shaped scatter diagrams.
For other diagrams, r can be misleading. Outliers and non-linearity are
problem cases.  
r measures linear association, not association in general.  
Ecological correlations are based on rates or averages. They are often used
in political science and sociology. And they tend to overstate the strength
of an association. So watch out.  
Correlation measures association. But association is not the same as
causation. It may only show that both variables are simutaneouly influenced
by some third variable.  

### 10. Regression
The regression method describes how one variable depends on another.  
Associated with each increase of one SD in x there is an increase of only r
SDs in y, on the average.  
In virtually all test-retest situations, the bottom group on the first test
will on average show some improvement on the second test, and the top group
will on average fall back. This is the _regression effect_.  
 
### 11. The R.M.S Error for Regression
The r.m.s. error for regression says how far typical points are above or
below the regression line.  
The r.m.s. error is to the regression line as the SD is to the average.  
The r.m.s. error for the regression line of y on x can be figured as  
                \sqrt{1 - r^2} \times the SD of y.  
Prediction errors are often called residuals. The residuals average out to 0
;and the regression line for the residual plot is horizontal.  
Suppose that a scatter diagram is football-shaped (honoscedastic). Take the
points in a narrow vertical strip. They will be off the regression line (up
or down) by amounts similar in size to the r.m.s. error. If the diagram is
heteroscedastic, the r.m.s. error should not be used for individual strips.  
### 12. The Regression line
Among all lines, the one that makes the smallest r.m.s. error in predicting
y from x is the regression line.  

### 13. What Are the Chances?

### 14. More about Chance
The addition rule finds the chance that at least one of two things happens.  
The multiplication  rule finds the chance that two things both happen.  

### 15. The Binomial Formula
