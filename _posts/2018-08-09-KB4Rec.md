---
layout: post
title: "KB4Rec"
date: 2018-08-09
categories: [Machinelearning]
tags: [knowledge base, recommender system, deep learning]
use_math: true
---

## End-To-End Memory Networks

A novel recurrent neural network (RNN) architecture where the recurrence reads from a possibly large external memory multiple times before outputting
a symbol. 

The key concept of multiple computational hops yields improved results.

1. single layer case

## Key-Value Memory Networks for Directly Reading Documents

Knowledge Bases (KBs) have intrinsic limitations such as their inevitable incompleteness and fixed schemas that cannot support alarietie
To bridge the gap between using a KB and reading documents directly, the Key-Value Memory Network (KV-MemNN) is proposed, which can work with either knowledge
source. The KV-MemNN performs QA by first storing facts in a key-value structured memory before reasoning on them in order to predict an answer.
The memory is designed so that the model learns to use keys to address relevant memories with respect to the question, whose corresponding values are subsequently
returned.

Model Description

In KV-MemNNs we define the memory slots as pairs of vectors $$(k1; v1), \ldots, (kM; vM)$$ and denote the question $$x$$. 
The addressing and reading of the memory involves three steps:
* __Key Hashing__
* __Key Addressing__
* __Value Reading__

## Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks

Task: Sequential Recommendation 

RNN-based    By encoding historical interaction records into a hidden state vector (called sequential preference representation), it is possible for these
methods to capture dynamic user preference over time and measure the likelihood of the next item. Since the state vector is encoded in a highly abstractive way,
it is difficult to capture or recover fine-grained (e.g., attribute or feature level) user preference from the interaction sequence. Furthermore,
the latent vector representation is usually hard to understand and explain. In other words, it lacks of interpretability.

